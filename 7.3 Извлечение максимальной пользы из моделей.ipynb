{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7.3 Извлечение максимальной пользы из моделей.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN3YoTsIc4MNcTIeJk0hV/u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"K4TesJMh-NTV"},"source":["# **Шаблоны улучшенных архитектур**"]},{"cell_type":"markdown","metadata":{"id":"49cs5gjE-QBo"},"source":["## Пакетная нормализация"]},{"cell_type":"markdown","metadata":{"id":"zdN8C1BE-XNu"},"source":["В предыдущих примерах нормализация выполнялась перед передачей данных в модели. Однако нормализация должна проводиться после каждого преобразования,\n","выполняемого сетью: даже если данные на входе в сеть Dense или Conv2D имеют\n","среднее значение 0 и единичную дисперсию, нет оснований полагать, что то же\n","самое можно будет сказать в отношении данных на выходе.\n","\n","Пакетная нормализация — это тип слоя (BatchNormalization в Keras), введенный в 2015 году Сергеем Йоффе (Sergey Ioffe) и Кристианом Сегеди (Christian\n","Szegedy)1\n","; он может адаптивно нормализовать данные, даже если среднее и дисперсия изменяются во время обучения. Его принцип действия основан на вычислении экспоненциального скользящего среднего и дисперсии данных, наблюдаемых\n","в процессе обучения. Основное назначение пакетной нормализации — помочь\n","распространению градиента подобно остаточным связям и дать возможность\n","создавать более глубокие сети. Некоторые глубокие сети могут обучаться, только\n","если они включают в себя несколько слоев BatchNormalization. Например, слои\n","BatchNormalization широко используются во многих продвинутых архитектурах сверточных нейронных сетей, входящих в состав Keras, таких как ResNet50,\n","Inception V3 и Xception.\n","\n","Обычно слой BatchNormalization используется после сверточного или полносвязного слоя:"]},{"cell_type":"code","metadata":{"id":"UcGxwN0s-PZA"},"source":["conv_model.add(layers.Conv2D(32, 3, activation='relu'))\n","conv_model.add(layers.BatchNormalization())               # После слоя Conv\n","\n","dense_model.add(layers.Dense(32, activation='relu'))\n","dense_model.add(layers.BatchNormalization())              # После слоя Dense\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"STj0OhFX-5RB"},"source":["Слой `BatchNormalization` принимает аргумент `axis`, определяющий ось признаков для нормализации. По умолчанию этот аргумент принимает значение `–1`, что соответствует последней оси во входном тензоре. Это правильное значение,\n","когда используются слои `Dense, Conv1D`, рекуррентные слои и слои `Conv2D` со значением `\"channels_last\"` в аргументе `data_format`. Но в слоях `Conv2D` со значением\n","\"`channels_first`\" в аргументе `data_format` ось признаков — это ось с индексом 1;\n","поэтому в таких случаях слою `BatchNormalization` следует передавать число 1\n","аргументе axis."]},{"cell_type":"markdown","metadata":{"id":"OclGUAF8_i9Y"},"source":["## **ПАКЕТНАЯ РЕНОРМАЛИЗАЦИЯ**"]},{"cell_type":"markdown","metadata":{"id":"J3EeG0Cf_UHL"},"source":["Не так давно, в 2017 году, Сергеем Йоффе был предложен более совершенный метод, нежели обычная пакетная нормализация, — метод пакетной ренормализации1\n",".\n","Он предлагает очевидные преимущества по сравнению с пакетной нормализацией, без\n","дополнительных накладных расходов. На данный момент еще рано говорить, вытеснит\n","ли он пакетную нормализацию, но я думаю, что это вполне возможно. А совсем недавно\n","Гюнтер Кламбаер (Günter Klambauer) с коллегами представили самонормализующиеся\n","нейронные сети2\n",", которые позволяют сохранять данные в нормализованном состоянии\n","после прохождения через любой слой Dense за счет использования специальных функций активации (selu) и инициализации (lecun_normal). Эта схема выглядит довольно\n","интересной, но пока она ограничивается только полносвязными сетями и ее полезность\n","на данный момент не подтверждена широкими исследованиями."]},{"cell_type":"markdown","metadata":{"id":"w6nMua5J_2EM"},"source":["## Раздельные свертки по глубине"]},{"cell_type":"markdown","metadata":{"id":"O_Joks9WA6JR"},"source":["Что бы вы подумали, если бы я сказал, что существует такой слой, который можно\n","использовать взамен Conv2D и с помощью которого сделать модель более легкой\n","(с меньшим количеством обучаемых весовых параметров) и быстрой (с меньшим\n","количеством операций с вещественными числами), а также повысить качество\n","решения задачи на несколько процентных пунктов? Всеми перечисленными качествами обладает слой раздельной свертки по глубине (SeparableConv2D). Этот\n","слой выполняет пространственную свертку каждого канала во входных данных\n","в отдельности перед смешиванием выходных каналов посредством поточечной\n","свертки (свертки 1 × 1), как показано на рис. 7.16. Это эквивалентно раздельному выделению пространственных и канальных признаков, что оправданно,\n","если предполагается сильная корреляция пространственных местоположений\n","на входе, но разные каналы практически не зависят друг от друга. Он требует\n","намного меньше параметров и выполняет меньше вычислений, благодаря чему\n","получаются более быстрые модели с меньшими размерами. И поскольку это более\n","репрезентативно эффективный способ выполнения свертки, он позволяет получать более качественные представления с меньшим объемом исходных данных\n","и, соответственно, более качественные модели.\n","\n","Эти преимущества особенно важны для обучения небольших моделей с нуля на\n","ограниченном наборе данных. Например, вот как можно построить легковесную\n","нейронную сеть со сверткой по глубине для решения задачи классификации изображений (с применением классификатора softmax) при небольшом объеме обучающих данных:"]},{"cell_type":"code","metadata":{"id":"il3OcM_t-keo"},"source":["from keras.models import Sequential, Model\n","from keras import layers\n","\n","height = 64\n","width = 64\n","channels = 3\n","num_classes = 10\n","\n","model = Sequential()\n","model.add(layers.SeparableConv2D(32, 3,\n","                                activation='relu',\n","                                input_shape=(height, width, channels,)))\n","model.add(layers.SeparableConv2D(64, 3, activation='relu'))\n","model.add(layers.MaxPooling2D(2))\n","model.add(layers.SeparableConv2D(64, 3, activation='relu'))\n","model.add(layers.SeparableConv2D(128, 3, activation='relu'))\n","model.add(layers.MaxPooling2D(2))\n","model.add(layers.SeparableConv2D(64, 3, activation='relu'))\n","model.add(layers.SeparableConv2D(128, 3, activation='relu'))\n","model.add(layers.GlobalAveragePooling2D())\n","model.add(layers.Dense(32, activation='relu'))\n","model.add(layers.Dense(num_classes, activation='softmax'))\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uhghY0KmBZrM"},"source":["В отношении крупных моделей раздельные свертки по глубине составляют основу\n","архитектуры Xception высококачественных сверточных нейронных сетей, входящей в состав Keras. Узнать больше о теоретических основах раздельной свертки\n","по глубине и архитектуре Xception можно в моей статье «Xception: Deep Learning\n","with Depthwise Separable Convolutions»1\n","."]},{"cell_type":"markdown","metadata":{"id":"8GCKpvn6BbXW"},"source":["# **Оптимизация гиперпараметров**"]},{"cell_type":"markdown","metadata":{"id":"1kVJk2WWFiqu"},"source":["Проблемы сложны, а область еще молода, поэтому в настоящее время в нашем\n","распоряжении имеется весьма ограниченный набор инструментов для оптимизации моделей. Часто случайный поиск (многократный выбор случайных значений\n","гиперпараметров) оказывается лучшим решением, несмотря на то что он самый\n","простой. Однако я обнаружил один инструмент, который достоверно лучше случайного поиска, — Hyperopt (https://github.com/hyperopt/hyperopt), библиотеку\n","на Python для оптимизации гиперпараметров: внутренне она использует деревья\n","оценок Парзена для предсказания наборов гиперпараметров, которые с высокой\n","вероятностью дадут положительный результат. Еще одна библиотека, с названием\n","Hyperas (https://github.com/maxpumperla/hyperas), интегрирует Hyperopt для использования с моделями Keras. Обязательно обратите на нее внимание."]},{"cell_type":"markdown","metadata":{"id":"WshpYkfwFkHH"},"source":["# **Ансамблирование моделей**"]},{"cell_type":"markdown","metadata":{"id":"1C2XJoR5Fq5p"},"source":["Возьмем в качестве примера задачу классификации. Самый простой способ объединить прогнозы из множества классификаторов (ансамблировать классификаторы) — получить среднее их прогнозов на этапе вывода:"]},{"cell_type":"code","metadata":{"id":"OXqkqlHs-khi"},"source":["preds_a = model_a.predict(x_val)        #Применение четырех разных моделей для вычисления начальных прогнозов\n","preds_b = model_b.predict(x_val)\n","preds_c = model_c.predict(x_val)\n","preds_d = model_d.predict(x_val)\n","\n","final_preds = 0.25 * (preds_a + preds_b + preds_c + preds_d) # Этот новый массив прогнозов должен получиться\n","                                                              # более точным, чем любой из начальных"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0J03uz7rF5QR"},"source":["Этот прием даст положительные результаты, только если исходные классификаторы примерно одинаково хороши. Если один будет значительно хуже других,\n","окончательный прогноз может получиться хуже прогноза лучшего классификатора\n","в группе."]},{"cell_type":"markdown","metadata":{"id":"fPfjqivrGCiU"},"source":["Более эффективный способ ансамблирования классификаторов — вычисление\n","взвешенного среднего с определением весов по проверочным данным, когда лучший классификатор получает больший вес, а худший — меньший. Для поиска\n","оптимальных весов в ансамбле можно использовать алгоритм случайного поиска\n","или простой оптимизации, такой как **Nelder-Mead:**"]},{"cell_type":"code","metadata":{"id":"AVZLRzWlGCJz"},"source":["preds_a = model_a.predict(x_val)\n","preds_b = model_b.predict(x_val)\n","preds_c = model_c.predict(x_val)\n","preds_d = model_d.predict(x_val)\n","\n","final_preds = 0.5 * preds_a + 0.25 * preds_b + 0.1 * preds_c + 0.15 * preds_d         #Эти веса (0.5, 0.25, 0.1, 0.15),\n","                                                                                        # как предполагается, получены эмпирическим путем\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6PL0YCQwHILO"},"source":["Существует много возможных вариантов: вы можете вычислить среднее экспоненциальное прогнозов, for instance. В общем случае простое взвешенное среднее\n","с весами, оптимизированными на проверочных данных, может служить очень неплохим базовым решением.\n","\n","Ключом к достижению успеха в результате ансамблирования является разнообразие набора классификаторов. Разнообразие — это сила. Если бы все незрячие\n","мудрецы ощупали только хобот слона, они согласились бы, что слоны похожи на\n","змей, и навсегда бы остались в неведении о действительной форме слона. Разнообразие — вот что обеспечивает успех ансамблирования. Выражаясь языком\n","машинного обучения: если все ваши модели будут одинаково предвзятыми, ваш\n","ансамбль сохранит эту предвзятость. Если ваши модели будут предвзяты поразному, предвзятости будут нивелировать друг друга, и ансамбль получится\n","более точным и надежным.\n","\n","По этой причине объединяться в ансамбли должны максимально хорошие и разные\n","модели. Это обычно означает использование разных архитектур или даже разных подходов к машинному обучению. Единственное, пожалуй, чего не следует\n","делать — ансамблировать ту же сеть, обученную несколько раз, даже при разных\n","начальных случайных значениях. Если ваши модели различаются только начальными значениями и тем, в каком порядке они обрабатывали обучающие данные,\n","ваш ансамбль будет иметь низкое разнообразие и обеспечит лишь незначительное\n","улучшение по сравнению с единственной моделью."]},{"cell_type":"markdown","metadata":{"id":"-Vb27CpzHL40"},"source":["В своей практике я обнаружил один прием, дающий хорошие результаты, — однако он не является универсальным и подходит не для всякой предметной области — использование ансамбля древовидных методов (таких, как случайные\n","леса или деревья градиентного роста) и глубоких нейронных сетей. В 2014 году Андрей Колев (Andrei Kolev) и я вместе заняли четвертое место среди решений\n","задачи обнаружения бозона Хиггса на сайте Kaggle (www.kaggle.com/c/higgsboson), использовав ансамбль разных древовидных моделей и глубоких нейронных сетей. Примечательно, что одна из моделей в ансамбле реализовала другой\n","метод (это был регуляризованный жадный лес — regularized greedy forest) и имела\n","существенно худшую оценку, нежели остальные. Неудивительно, что она получила маленький вес в ансамбле. Тем не менее, к нашему удивлению, оказалось,\n","что она значительно улучшала ансамбль в целом, потому что сильно отличалась\n","от всех других моделей: она сохраняла информацию, к которой другие модели\n","не имели доступа. Именно в этом заключается суть ансамблирования. Главное\n","не то, насколько хороша ваша лучшая модель, а то, насколько разнообразны модели-кандидаты.\n","\n","В последнее время большим успехом на практике пользовалась обширная категория моделей, сочетающих глубокое и поверхностное обучение. Такие модели\n","состоят из совместно обучаемых глубокой нейронной сети и большой линейной\n","модели. Совместное обучение семейства разных моделей — еще один способ ансамблирования."]},{"cell_type":"code","metadata":{"id":"bozmswks-klD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y8UnfssK-koH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nc8RwcsW-krD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-MHp14H4-kt0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uCCcZ-zA-kwi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PhMrs6jj-k0L"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jMvUWK5y-k3t"},"source":[""],"execution_count":null,"outputs":[]}]}